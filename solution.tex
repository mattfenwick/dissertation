\chapter{Solution: An Approach for Reproducible Analysis}


\section{A Model for Reproducible NMR}

A data model is a means of specifying the structure of information  
\cite{codd1970relational}.  This
information may be used as inputs and outputs for computational tools, or
it may be archived and available for reference use.  Data models are useful
because they provide a formal specification of the structure, which enables
unambiguous, correct, and automated use of data.  Data models are
abstract specifications; they must be implemented in source code in order
to become a usable artifact.

This section will cover a data model for reproducibility.  Once a data
model exists, it can be implemented as part of a software program that
facilitates reproducible data analysis, as will be covered in a later 
chapter.  The core of this data model is formed by the BMRB \cite{bmrb}
and CCPN \cite{ccpn} data models.  These models are then extended with
several additional data types and properties in order to enable 
reproducibility.


\subsection*{Deductive reasoning}
When a data feature is interpreted, a deductive rule is used to provide
the result, given the input.  In order to support the capture of this data, 
a model both for the application of a rule to a data set, and for the rules
themselves, was created.  The rules are modeled as an extensible library of 
commonly used deductive reasons.  Modeling the rules as an enumerated library 
enables quick and easy use.  During analysis, one or more rules are applied to 
make a deduction.  This is shown in Figure \ref{deduction_model}.

Using a rule-based system is based upon prior work in computational fields
\cite{buchanan1984rule, reiter1987theory}, with systems that have been used for 
medical diagnostic, industrial fault detection, and e-mail spam filtering.
Important characteristics include the system's ability to support and describe
probabilistic reasoning, learning and tutoring, future extensions, explanations
of past analyses, and performance evaluations \cite{buchanan1984rule}.
The content of the deductive rule library is based on 
established practices during data analysis \cite{guerry2011automated, hncacb,
hnco, cbcaconh, hbhaconh, picky, xeasy, sparky, ccpn}.
It is presented in Appendix \ref{sec_library}.



\subsection*{Intermediate results}
The general outline of the solution is a model of the process of data analysis,
consisting of a sequence of snapshots of the data set, taken at carefully chosen 
moments during analysis, which show the full process of analysis by capturing
all changes.  Each snapshot -- other than the first -- contains a link to
the previous snapshot, as well as a set of data differences.  The differences
between snapshots provide the key value of this solution: they explicitly
show how the data set is changed over time.
Associated with each snapshot is a small amount of meta data to help describe
the snapshot, including a timestamp, author information, and a deductive
annotation, which describes the "why" of the changes and will be covered in
the next section.

The core of the strategy is based on that used by Version Control System (VCS) 
software tools \cite{vcs_concepts, hinsen2009vcs}, which are commonly 
applied for managing source code of 
software projects \cite{loeliger2012git, cvs, svn}.
These tools were originally implemented in order to manage the change in 
source code over time, while retaining the ability to easily inspect past 
states of the code.  It was found that application of such tools led to 
large increases in productivity, robustness, correctness, and reduced 
faults \cite{fischer2003vcs}.

While the general solution is adapted from version control software, in order
to effectively capture intermediate NMR data sets, such that the process of
analysis is clear and understandable, the solution must be augmented to fit
the specific needs of the NMR domain.
In other words, capturing meaningful intermediate snapshots is challenging; it 
is not sufficient to capture them indiscriminately.  If snapshots are captured 
too infrequently, the situtation is not significantly different from current 
practices: the analysis process will not be reproducible.  If too many snapshots
are captured, reconstructing the logical dependencies will not be possible;
in addition, the valuable information may be difficult to identify compared to
the large amounts of useless information.  A third potential problem is 
collecting snapshots indiscriminately -- i.e. not in a manner that corresponds
to the actual process employed.  This, too, prevents later use of the 
intermediate data because the process has not been correctly captured.

Therefore, there are several principles of intermediate data collection which
must be observed in order to create a useful data set.  These principles help
to ensure that snapshots are created neither too often nor too rarely, and
that they are useful for future perusal:
\begin{itemize}
  \item time.
    Snapshots should be taken often enough that all modifications are captured.
    For example, when peaks are initially picked by an automated tool, and then 
    modified (perhaps sorting them into signal, noise, and artifact classifications)
    by manual adjustment, a snapshot must be taken immediately after the automated
    peak picker is run, and before any modifications are made.  When additional
    modifications are made, it is again necessary to take another snapshot before
    these changes, in order to capture the previous state of the data set -- which
    is otherwise lost if this is not done.
  \item content.
    Each snapshot should have a clear and simple focus on analyzing a single
    feature or performing a single type of interpretation.  For example, a snapshot
    should not include changes both to resonance typing and to peak lists if those
    changes are not inter-dependent.  % this ensure that logical dependencies are recoverable ? ... 
  \item cohesion.
    Similarly to the previous point, changes which are inter-dependent belong in
    the same snapshot.  For example, when assigning GSSs sequentially, assume there
    are two potentially matching GSSs based on CA and CB resonances, which however
    have not been specifically assigned i/i-1.  If one GSS is determined to be the
    first, and the other the second, then the CA(i), CA(i-1), CB(i), and CB(i-1)
    assignments of the resonances in both spin systems are determined.  These 
    changes all naturally belong in a single snapshot, since they are logically 
    co-dependent.
  \item logical dependencies must be recoverable.
    The previous two points enable recovery of deductive, logical dependencies.  
    The sequential process of deduction is the core of manual analysis, and 
    therefore it is important to capture it clearly.  This means that the 
    dependencies must be reconstructable from the sequence of intermediate data 
    sets.
\end{itemize}

% devote more space to discussing diffs/comparisons, logical dependencies
%   what, why, and how.  
%   also, how the VCS-based solution supports these goals
The primary goal of capturing intermediate data sets is to 
facilitate reproducibility by modeling and saving the process of analysis.  
A system which does so by capturing a sequence of snapshots of the data set 
at intermediate timepoints meets the requirements for reproducibility.
First, such a system is able to correctly recapitulate the changes over time
due to manual and automated analysis.  For example,
if an automated peak picker were used on a spectrum, and then the results
manually verified and corrected, if snapshots were taken at the appropriate
moments, the sequence of snapshots would show both the complete results of
the automated tool, as well as every manual change made.
% TODO should I say more here?
Second, by capturing the full context of each modification, the logical and 
temporal dependencies between various features of the data set are trackable.
% TODO what? the following doesn't make much sense
Additionally, capturing multiple intermediates allows rich comparisons to be made 
between snapshots.  These comparisons, combined with the deductive annotations,
indicate the changes made with each specific deductive reason. 


\subsection*{Extraneous results}
Our approach is to allow any number of peaks and GSSs, and to 
augment them with additional data fields which distinguish between signal, 
noise, contaminants, etc.  This allows one to make a critical distinction 
between: 1) finding/recording a peak based purely on characteristics of 
the spectrum such as volume, height, relative height compared to noise, 
lineshape, and linewidth, and 2) interpreting a peak as signal, noise, 
etc. (and the same for GSSs).  Even peaks and GSSs for 
which no analysis is made can be kept in the data set without encumbering 
assignment of true peaks and GSSs.

To model extraneous results, the BMRB and CCPN models \cite{bmrb, ccpn}
were extended to support additional fields which distinguished between 
extraneous and primary data.
This applies to peaks, resonances, and GSSs.
The general approach for using this model is to never directly delete
a peak, resonance, or GSS, but rather to mark it as extraneous by modifying
its associated category from 'signal' to 'artifact', 'noise', 'contaminant'
etc.

For example, while using an interactive spectral analysis such as Sparky or
CCPN Analysis \cite{sparky, ccpn} for peak picking a spectrum, it is common
to run the automated, built-in peak picker and then to manually correct the
results by deleting some peaks and adding new ones.  This approach works
differently; peaks are not deleted.  If the category of each of the
peaks initially picked by the automated tool is 'signal', then the
user must correct all of the categories for peaks 
determined to be signal or noise; note that these peaks are not deleted
from the list.  They remain in the list but with a different category
tag that differentiates them from signal peaks.

A further category of extraneous data is peaks from amino acid sidechains; 
it is presented in Tables \ref{nhsqc_peaktypes}, \ref{hnco_peaktypes}, 
\ref{hncacb_peaktypes}, \ref{hbhaconh_peaktypes}, \ref{cconh_peaktypes}, and
\ref{hcconh_peaktypes}.  While most of the
peaks in these experiments correspond to backbone covalently bond atom groups, 
and are used for backbone sequential assignments, many sidechain GSSs are
visible as well.  Although these peaks are often ignored, they do contain
useful information.  They also can confound analysis if they are not properly
recognized as sidechain peaks.  Lastly, their presence is surprising to 
newcomers to the analysis process, as they are typically not explicitly 
recognized as part of the standard experiments.

\subsection*{Notes}


\section{Discussion}
Makes biases explicit
How general is the strategy?



\chapter{Reproducible NMR Data Set}

In order to prove the validity of the approach described in the previous
chapter, it was applied to typical NMR data in order to solve a protein
structure.  The process was carried out in full, and sufficient data was
captured during the process in order to render it reproducible.


\section{Methods and Materials}

Time-domain data of Samp3, a Ubiquitin-like protein, were kindly provided by 
Dr. Mark Maciejewski.  The data were processed to frequency-domain spectra
using NMRPipe, and then analyzed first in CCPN Analysis \cite{ccpn} and later 
in Sparky \cite{sparky}.  A preliminary analysis process used CCPN Analysis,
along with git and several Python utilities to create and manage the 
reproducible history.  The final analysis used Sparky, the model and approach
described in the previous chapter, and the extension described in the following
chapter.  CYANA was used to assign NOEs,
disambiguate stereospecific assignments, and calculate structure bundles.
% TODO more detail?  pictures?


\section{The analysis process}
Starting from time-domain data sets, I carried out the standard data analysis
process using a subset of the CCPN data model \cite{ccpn}, 
with the additional property that the process itself was captured as
a series of annotated snapshots, and extraneous results were recorded as well.
The full data set including complete annotated snapshot history may be found at 
\url{https://github.com/mattfenwick/samp3}
(a previous version using the CCPN Analysis program may additionally be found
at \url{https://github.com/mattfenwick/PeakPicker}).



\section{Archiving reproducible data sets}
A major portion of the value derived from collecting reproducible data sets
is disseminating them so that others may obtain and use or inspect the data
in some way.  The standard means for sharing NMR-derived data is the BMRB
\cite{bmrb}, which uses the NMR-STAR file format.  In order to enable archival
of reproducible data sets, we have collaborated with the BMRB to extend the
NMR-STAR data dictionary, so that reproducible data sets may be collected and
deposited in the NMR-STAR format.
The NMR-STAR data dictionary, which may be found at 
\url{http://www.bmrb.wisc.edu/dictionary/}, catalogs the names, structure,
intended use, and definitions of the data types handled by the BMRB.
The git repository may be extracted and converted to a single NMR-STAR file;
code for doing this may be found at 
\url{https://raw.githubusercontent.com/CONNJUR/Samp3-extractor/master/samp3_extractor.sh}:
\begin{verbatim}
filepath="sparky_data.json"
myids=( $(git log --format="%H" $filepath) )
 
for (( i=0,j=1; i < ${#myids[@]}; i++,j++ )) 
    do
        name="temp/a$j.txt"
        sha=${myids[$i]}
        git show $sha:$filepath > $name
    done
\end{verbatim}
This is a Bash shell script which uses the git commands "log" and "show" to 
first locate identifiers for
each version of the JSON-formatted dump file, then to read the contents of
those versions from the git database, producing new files, which are stored
in the "temp/" directory.  A Python program is then used to load the file
contents into memory, and perform a semantic diff algorithm which recognizes
differences between file versions.  The final output is in the extended
NMR-STAR format.




\section{Discussion}
Applying the reproducibility approach, described in the previous chapter, first
and foremost proves that the approach is viable: that it can be used to 
effectively analyze NMR data, and lead to a final data set which is usable.  
The approach I have applied is not the only one 
which could possibly be employed; rather, it is one of several.  The principles
embodied in the approach are to make data explicit -- whether that data is the
context of a deduction, or the deduction itself, or the motivation behind a
deduction -- and to capture that data efficiently.  In this respect, it is 
similar to recording the process of analysis in a lab notebook, except that 
the data requirements for the electronic "lab notebook" are far larger.
The overal proof of the effectiveness of such an electronic lab notebook is
that the relevant information is explicitly captured, such that it is made
available for later perusal.  This approach does effectively capture such 
information.  This ground-breaking work also shows how to
use the approach, including possible pitfalls;  the final, reproducible data
set is valuable in that it shows what can be achieved through a reproducible
approach, as well as creating a platform for future work towards enhanced
reproducibility.


% tables
\clearpage
\section{Tables}

\begin{table}[h]
    \begin{tabular}{ | c || c | c | c | c | c |}
    \hline
      Name              &  Dimension 1  &  Dimension 2  &  Dimension 3  &  Is NOESY?    \\    \hline
      NHSQC             &  H            &  N            &               &  No           \\    \hline
      HNCO              &  H            &  N            &  C            &  No           \\    \hline
      HNCACB            &  H            &  N            &  C            &  No           \\    \hline
      C(CO)NH-Tocsy     &  H            &  N            &  C            &  No           \\    \hline
      HBHA(CO)NH        &  H            &  N            &  H            &  No           \\    \hline
      HC(CO)NH-Tocsy    &  H            &  N            &  H            &  No           \\    \hline
      HCCH-Tocsy        &  H            &  C            &  H            &  No           \\    \hline
      hbCBcgcdHD        &  H            &  C            &               &  No           \\    \hline
      hbCBcgcdceHE      &  H            &  C            &               &  No           \\    \hline
      NOESY-NHSQC       &  H            &  N            &  H            &  Yes          \\    \hline
NOESY-CHSQC (D\textsubscript{2}O) & H   &  C            &  H            &  Yes          \\    \hline
      Aromatic NOESY    &  H            &  H            &               &  Yes          \\    \hline
 Aromatic GNOESY-CHSQC  &  H            &  C            &  H            &  Yes          \\    \hline
    \end{tabular}
    \caption[Spectra used in Samp3 analysis.]
            {Spectra used in Samp3 analysis.  The first four spectra were used
             for sequential assignment, the next three for aliphatic sidechain
             assignment, the next two for connecting aromatic sidechain protons
             to the aliphatic backbone, and the last four (all NOESYs) for
             obtaining distance restraints.}
    \label{samp3_spectra}
\end{table}




\chapter{Sparky Extension for Reproducible Spectral Analysis}

Sparky \cite{sparky} is a popular program for interactive peak picking,
GSS construction, and chemical shift assignment.  Sparky is implemented 
with a C++ core, and Python extensions.  It is designed with
extensibility in mind, with a convenient Python interface through which 
the core data model can be accessed.  The
extensions are also able to augment the user interface with additional
controls, as well as script common operations, and provide extra algorithms
for analysis.  Since Python is a full-featured programming language, 
it is also possible to interact with the filesystem, loading and dumping
data if necessary, as well as calling additional third-party tools.

The extension is intended to help the spectroscopist to capture the 
missing data of analysis: intermediate primary data, extraneous
data, deductive meta data, and notes.  The general approach is to augment
Sparky's data model and user interface with new functionality.



\section{Getting started with Sparky}

\subsection*{Getting Sparky}
A Sparky version including the reproducibility extension can be found at
\url{https://github.com/connjur/SparkyExtensions/releases}.  Simply 
choose the latest version of the correct platform, download it, untar and 
unzip it, and run the sparky executable (in Contents/Resources/bin in the
Mac version, and bin/ in the Linux version).

\subsection*{Dependencies}
The reproducibility extension requires a working git installation in order
to capture snapshots.  git is a freely available tool for version control,
and may be found at \url{http://git-scm.com/downloads}.
git works with local files -- no setup of remote hosts is required.
A git client (a list of which may be found at \url{http://git-scm.com/downloads/guis})
is a useful tool to help view and manage a git repository.  A simple,
cross-platform git client that I have used successfully is "giteye",
which may be found at \url{http://www.collab.net/giteyeapp}.

\subsection*{Sparky manual}
Sparky manuals, which cover the complete use of the program and its many
features in depth, may be found at 
\url{http://www.cgl.ucsf.edu/home/sparky/manual/} and 
\url{http://pine.nmrfam.wisc.edu/PINE-SPARKY/}.


\section{Concepts}
Automated algorithms do not usually produce perfect results in NMR analysis.
Automated peak picking,
GSS construction, and GSS-residue assignment need manual validation and
correction.  These manual modifications are inherently difficult, tedious,
and error-prone because they are the most complicated to correctly analyze.
However, correctly analyzing them has an important impact on the quality
of the final result.
The main goal of this extension is to facilitate reproducibility
by capturing the entire analysis process, including manual modifications.

The process of manual analysis is composed of a series of discrete steps.
At each step, a deductive rule is applied to the data set, producing a
modified new data set.  Thus, for each rule, knowing the context is important:
it allows one to determine how appropriate the application of a specific rule
is, as well as what the results should be, and to determine whether the 
results are consistent with expectations.

\subsection*{GSS and resonance}
Sparky's data model is shown in Figure \ref{sparky_model}.  It includes 
entities for spectra, peaks, resonances, GSS, atoms, and molecules, among
others.  Sparky does not natively distinguish between a GSS (generic spin 
system) and a residue, nor between a resonance and an atom.

Both GSSs and resonances have been implemented in the extension, based 
on the CCPN model \cite{ccpn}.

\subsection*{Extraneous data}
In standard analysis, false positive peaks (picked as true peaks initially,
but later determined to be noise or artifactual) and false positive spin 
systems (not assigned to any residue) are typically deleted and/or ignored.
In this extension, \textbf{data is never deleted}.  Such results are kept,
since they have valuable information content, even if they are not used for
the immediate purpose of the analysis process (be it structure or dynamics
determination).  These results are kept by, instead of deleting them, 
explicitly marking them as not of interest, and placed in a different
category.

\subsection*{Snapshots}
Snapshots of intermediate states during analysis are used to enable rehashing
of the deductive process.  In general, by capturing a snapshot of the data set
each time a deductive rule is applied, the context of each manual modification
is captured.

An additional benefit of capturing snapshots is that they allow one to go 
backward in time.  This is useful for understanding what happened and
why, but is also useful for fixing mistakes and accidents -- this is
important because Sparky can not undo changes.
If a datum is accidentally changed, it may not even be noticed, and it is 
impossible to easily restore.  Capturing snapshots trivially solves both
of these problems.

\subsection*{Deductive reasoning}
Capturing the deductive rules applied during the process of manual assignment
provides semantic information about what is being done and why.  This makes it
far more meaningful to examine the sequence of snapshots of an analysis
process and determine the context.

