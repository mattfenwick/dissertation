\chapter{Software for Practical, Reproducible Analysis}

\section{Sparky extension}

\section{NMR-Star library}
NMR-Star is the file format used by the BMRB \cite{bmrb} for archival of
NMR data.  As such data is useful for further studies, and archiving
data in the BMRB is the primary means of dissemination, it is important
to be able to work with NMR-Star files.

This library facilitates reproducibility by providing a robust interface
for working with NMR-Star files.  It allows both the creation of NMR-Star 
files as well as extraction of data -- for further querying -- of existing
files.  It is used by the previously mentioned Sparky extension.

To handle these files, a library was implemented both in Java 
\cite{fenwick2013} and in Python.  This library provides capabilities both
for reading and for writing NMR-Star files.  Although several tools for
dealing with NMR-Star files had already been implemented \cite{ccpn, bmrb},
there are several attributes of this library which set it apart:
\begin{itemize}
  \item error reporting of illegal input.  When malformed input is encountered,
    a useful, location-specific error is reported which includes sufficient
    information to quickly pinpoint and diagnose the problem.
  \item complete, standards-compliant NMR-Star syntax definition.
  \item open source under the MIT license.  This allows other interested 
    developers to peruse the source code to gain ideas, use the library in
    new applications, and modify and extend the library to fix problems or
    add new features if necessary.
  \item low coupling.  As a simple library, in order to use it, the library
    is simply imported using standard language facilities in order to use
    it through its programmatic API.  It does not require any external tools
    or dependencies, reducing the barrier to setup and installation.  It does
    not require learning to use additional tools or languages, merely the 
    host language; it takes advantage of the native facilities for abstraction
    and composition provided by the host language.
  \item high cohesion.  The library provides a simple, focused interface.
    This means it is easy to learn and use because it only deals with parsing
    the concrete syntax of NMR-Star files.
\end{itemize}
The library is freely available online (
\url{https://pypi.python.org/pypi/NMRPyStar}, 
\url{https://github.com/CONNJUR/StarParser}
).


\section{Connjur: ST and WB}
The spectral reconstruction phase involves the processing of time-domain 
FID data to frequency-domain spectra.  There are several approaches for such
a transformation, including the Fourier Transform, Maximum Entropy reconstruction,
and Multi-dimensional decomposition \cite{nmrpipe, rnmrtk, mdd}.  There are
two key pieces to the data: the primary data (time-domain or frequency-domain),
which is typically in a binary format; and the meta data which records
important properties such as number of points, dwell time, and spectral width.

As covered earlier, spectral reconstruction involves the sequential application
of multiple functions, including linear prediction, zero fill, and apodization,
each of which must be parameterized appropriately, for the purposes of 
optimizing spectral characteristics such as peak shape, line width, and
signal-to-noise ratio.  In general, the exact effect of each of these operations
may depend on both the primary data as well as the meta data, and both types of
data may be modified as well.

Correct spectral reconstruction requires that the meta data is correctly 
handled, and reproducibility requires that all of the parameters are captured
as well (if the exact software versions used are captured, then it is not
necessary to capture the input and output primary and meta data from each
operation, since these can be regenerated as needed).  Two recent tools from
our lab, Connjur-ST \cite{connjur-st} and Connjur-WB, alleviate the
reproduciblity problem in the area of spectral reconstruction.

Connjur-ST, or Spectrum Translator, was designed with the general goal of
translating between various spectral formats, of which there are many, 
including Bruker, Varian/Agilent, XEasy, UCSF, RNMRTK, to name a few.  
It is necessary to convert between multiple formats during spectral 
reconstruction and analysis because different tools, each of which provides
valuable functionality, require different formats for input and output.
Thus, to use tools with differing format requirements, conversion will be
necessary.  While several tools do exist which implement specific conversions
between pairs of formats, there was previously no tool able to perform a
conversion between any two arbitrary formats.  The result was an artificial
restriction on combinations of tools, due to format constraints.  In addition,
attempting to remove this restriction by implementing additional tools is not
a satisfactory solution, because the number of tools required -- if each one
performs a single conversion -- grows with the square of the number of formats.
Such a solution clearly requires too much time and effort for initial
implementation, as well as future maintenance effort.

Spectrum Translator addresses this problem by means of a common data model,
which can be converted to and from any format.  For each format, a single
importer and a single exporter is required, which deal with conversion between
the format and Spectrum Translator's common data model.  This reduces the number
of converters required to the number of formats.  Thus, for translation between
any of five formats, the number of conversions required is reduced from 25 to
10 -- a 60\% reduction in the amount of conversions.  The discrepancy is even
greater when larger numbers of formats are considered.

This program is implemented in the Java programming language as an open source
library available from our website at \url{http://connjur.uchc.edu/downloads/st/}.
The advantage of using Java is that Java is "write once, run anywhere": Java
code, once compiled, may run on any Java Virtual Machine (JVM).  JVMs have been
implemented for many platforms, including Windows, Macintosh, and Linux.
This enables Spectrum Translator to run without modification on virtually any
computer.  A further advantage of Java is that as a popular programming 
language, there are many developers familiar with its syntax, semantics, 
class libraries, tooling, and deployment.

Spectrum Translator promotes data integration and consistency through the use
of a common data model and a single, unified conversion strategy.  It also
incorporates features for automatically reading meta data, which ease the
burden of meta data correctness for the user, which helps to ensure that the
meta data is more correct.  By applying a single interface to any format
conversion, the program has a smaller learning curve compared to learning
multiple differing interfaces for multiple tools.

Spectrum Translator has recently been extended to support non-uniform time-domain
data.  As the Rowland NMR Toolkit format is required in order to use its 
implementation of Maximum Entropy reconstruction \cite{rnmrtk}, Spectrum
Translator is an important enabler of the use of the technique, helping to 
make non-uniform data collection a realistic possibility for users who might
otherwise face significant hurdles in tooling.


\section{Sample Scheduler}
The creation of effective sample schedules is an important aspect of efficient,
non-uniform data collection 
\cite{maciejewski2011random, rovnyak2004accelerated, mobli2010non}.  There
are multiple strategies for data collection.  The strategy used to generate
a sample schedule and the exact sample schedule used to collect time-domain
data has an effect on the quality of the data and on the ease of later 
analysis, due to properties such as artifacts (described by the point-spread
function), resolution, and sensitivity (related to signal-to-noise ratio).

A tool has been implemented to reproducibly capture the parameterizations
used in sample schedule creation, and is available online at
\url{https://github.com/mattfenwick/PyScheduler}.  The tool features a 
collection of popular algorithms for creating non-uniform sample schedules
with specific, desirable properties.  The algorithms are integrated within
a single uniform, consistent interface which allows all input parameters
and outputs to be captured and archived.

The tool also features a data model for formal, precise communication of
sample schedules.  The data model not only deals with non-uniform time delays,
but also with non-uniform quadrature \cite{maciejewski2011random} and
non-uniform numbers of transients.  To my knowledge, the latter aspect of
non-uniform data collection remains a relatively unexplored domain, into
which this tool provides novel data representation capabilities.


\section{Discussion and conclusions}

 - open source
 - reproducibility
 - bottom-up design: synergy, deals with complexity, extensibility
 - explicit data models
 

% figures
\clearpage
\section{Figures}


